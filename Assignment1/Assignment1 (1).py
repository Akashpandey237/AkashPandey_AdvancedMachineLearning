#!/usr/bin/env python
# coding: utf-8

# In[1]:


from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, LSTM

def create_and_train_model(hidden_layers=2, units=64, activation='relu', loss='binary_crossentropy'):
    model = Sequential(name=f'My_Model_{hidden_layers}layers_{units}units_{activation}_{loss}')
    model.add(Embedding(max_features, 128, name='embedding_layer'))
    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, name='lstm_layer'))
    
    for i in range(hidden_layers):
        model.add(Dense(units, activation=activation, name=f'dense_layer_{i+1}'))
        model.add(Dropout(0.2, name=f'dropout_layer_{i+1}'))  # Example of using dropout

    model.add(Dense(1, activation='sigmoid', name='output_layer'))
    model.compile(loss=loss, optimizer='adam', metrics=['accuracy'])
    
    model.fit(x_train, y_train, batch_size=batch_size, epochs=3, validation_data=(x_test, y_test))
    score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)
    return acc


# In[2]:


import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout
import pandas as pd

# Load and preprocess data
max_features = 20000
maxlen = 80
batch_size = 32

(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)
x_train = sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = sequence.pad_sequences(x_test, maxlen=maxlen)

# Function to create and train model
def create_and_train_model(hidden_layers=2, units=64, activation='relu', loss='binary_crossentropy'):
    # Clear any previous TensorFlow session
    tf.keras.backend.clear_session()
    
    model = Sequential(name=f'My_Model_{hidden_layers}layers_{units}units_{activation}_{loss}')
    model.add(Embedding(max_features, 128, name='embedding_layer'))
    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, name='lstm_layer'))
    
    for i in range(hidden_layers):
        model.add(Dense(units, activation=activation, name=f'dense_layer_{i+1}'))
        model.add(Dropout(0.2, name=f'dropout_layer_{i+1}'))  # Example of using dropout

    model.add(Dense(1, activation='sigmoid', name='output_layer'))
    model.compile(loss=loss, optimizer='adam', metrics=['accuracy'])
    
    model.fit(x_train, y_train, batch_size=batch_size, epochs=3, validation_data=(x_test, y_test))
    score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)
    return acc

# Experiment with different configurations
results = []

for hidden_layers in [1, 3]:
    for units in [32, 64, 128]:
        for activation in ['relu', 'tanh']:
            for loss in ['binary_crossentropy', 'mse']:
                print(f'Training model with {hidden_layers} hidden layers, {units} units, {activation} activation, {loss} loss')
                acc = create_and_train_model(hidden_layers, units, activation, loss)
                results.append((hidden_layers, units, activation, loss, acc))

# Save results
results_df = pd.DataFrame(results, columns=['Hidden Layers', 'Units', 'Activation', 'Loss', 'Accuracy'])
results_df.to_html('results_summary.html', index=False)


# In[19]:


get_ipython().run_line_magic('matplotlib', 'inline')

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the results from the HTML file
results_df = pd.read_html('results_summary.html')[0]

# Display the results
print(results_df)

# Plot the accuracy of different configurations
plt.figure(figsize=(14, 8))
sns.barplot(data=results_df, x='Units', y='Accuracy', hue='Hidden Layers')
plt.title('Model Accuracy by Units and Hidden Layers')
plt.savefig('accuracy_by_units_and_hidden_layers.png')
plt.show()

# Another example of a visualization
plt.figure(figsize=(14, 8))
sns.boxplot(data=results_df, x='Activation', y='Accuracy', hue='Loss')
plt.title('Model Accuracy by Activation Function and Loss Function')
plt.savefig('accuracy_by_activation_and_loss_function.png')
plt.show()


# In[20]:


Detailed Report Template
Hereâ€™s a template for your final report:

Assignment 1: Neural Networks
Introduction
The purpose of this assignment was to explore and extend the initial neural network model used on the IMDB dataset to improve its performance.

Methodology
We experimented with different configurations, including:

Number of hidden layers: 1, 3
Number of units: 32, 64, 128
Activation functions: relu, tanh
Loss functions: binary_crossentropy, mse
Results
The summarized results of our experiments are as follows:

Hidden Layers	Units	Activation	Loss	Accuracy
1	32	relu	binary_crossentropy	0.83332
1	32	relu	mse	0.82760
1	32	tanh	binary_crossentropy	0.81832
1	32	tanh	mse	0.83264
1	64	relu	binary_crossentropy	0.83096
1	64	relu	mse	0.82732
1	64	tanh	binary_crossentropy	0.81712
1	64	tanh	mse	0.82292
1	128	relu	binary_crossentropy	0.83152
1	128	relu	mse	0.82604
1	128	tanh	binary_crossentropy	0.82220
1	128	tanh	mse	0.81020
3	32	relu	binary_crossentropy	0.83204
3	32	relu	mse	0.83496
3	32	tanh	binary_crossentropy	0.82848
3	32	tanh	mse	0.82876
3	64	relu	binary_crossentropy	0.82940
3	64	relu	mse	0.83044
3	64	tanh	binary_crossentropy	0.82580
3	64	tanh	mse	0.81348
3	128	relu	binary_crossentropy	0.82928
3	128	relu	mse	0.81540
3	128	tanh	binary_crossentropy	0.80540
3	128	tanh	mse	0.82128
Visualizations
Accuracy by Units and Hidden Layers


Accuracy by Activation Function and Loss Function


Discussion
From the results, we observed that:

Using three hidden layers generally performed better than one hidden layer in terms of accuracy.
The relu activation function typically provided better performance compared to tanh.
The binary_crossentropy loss function generally outperformed mse in terms of accuracy.
Conclusion
The best performing model configuration was:

Hidden Layers: 3
Units: 32
Activation: relu
Loss: mse
This configuration achieved the highest validation accuracy of 0.83496.


# In[ ]:




